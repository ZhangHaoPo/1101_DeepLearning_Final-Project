# 1101_DeepLearning_Final-Project

**系級 : 資科碩一**
**學號 : 110753113**
**姓名 : 張皓博**


## **Introduction**
期中的提案是要用CNN做5G環境下(eMBB, URLLC, mMTC)封包分類，但後來實際的資料集有點難找到，因此主軸還是使用**CNN分類封包**，分類的封包會參考下列的論文，<span style="color:red"> **分類是否為暗網(Tor)封包，並且分類出到底為何種應用類型** </span>。DL模型使用最簡單的**全連接神經網路及CNN**，資料集與此篇論文相同，**因此最後的比較標準會與參考的論文做比較**。
## **Related Work**
[Arash Habibi Lashkari, Gurdip Kaur and Abir Rahali, "DIDarknet: A Contemporary Approach to Detect and Characterize the Darknet Traffic using Deep Image Learning", 10th International Conference on Communication and Network Security, November 2020.](https://dl.acm.org/doi/10.1145/3442520.3442521)


**KEYWORDS**
**darknet, darknet traffic, encrypted traffic, VPN, tor, deep learning, detection, characterization**



**Analyzing darknet traffic helps in early monitoring of malware before onslaught [29] and detection of malicious activities after outbreak [33]. Following motivations led to this research**




## Darknet Traffic Detection

Darknet 流量偵測2000年初就有人在做，起出用決策樹[31]，single-flow traffic[72]，建立連線的前四五個封包的大小及direction information[12,13]，session flow[68]，taxonomy and traffic rules for various activities in darknet[39]，correlating lowinteraction honeypot(蜜罐) with darknet traffic[3]。

2015年，Nishikaze等人擷取了在來源網路的子網路(subnet at source network)與暗網通訊的30,373,399個封包。他們創造了27個特徵向量用於流量分析包含(packet count, source IPa and Port, destination IP and Port)。透過執行hierarchical clustering and matching malware signatures with identified packets來辨別惡意(Malicious)的封包。然而，他們一樣無法辨別新的惡意軟以樣本因為他們在本地端擷取(capturing at the local tap)。Distributed Reflection Denial of Service 分散式反射阻斷服務(DRDoS)攻擊可被檢測透過提取其他資訊例如強度(Intensity)，速率(rate)和地理位置(geo-location)[20]。另一種嘗試是，**封包的總頻率、來源主機數量以及 TCP 和 UDP 協定的目標埠**(targeted port)，可用於檢測暗網流量。

2016年，Ben等人，對從暗網收集的類似攻擊模式進行分組，並使用時間序列來表徵這些攻擊模式的活動級別(activity level)。一項全面的調查也報告了使用Honeyd（低交互蜜罐 low interaction honeypot）用於部署暗網服務及時間序列技術來分析暗網流量[22]。

在2018年，Wang等人[69]，改進了現存的user profiling方法透過提取個人敏感性資料例如top names, number of attributes, email domain, distribution from the darknet。探索了暗網上的八個hidden markplaces，以創建用於mining text以識別新威脅的threar dictionary。




## **Limitations of Current Detection Techniques**

- Diverse applications and protocols like browsing, chat, email, file transfer, streaming, VOIP, and torrent are <span style="color:red">**not incorporated**</span> in the majority of past papers
- <span style="color:red">**None**</span> of the papers characterized <span style="color:red">**VPN and Tor**</span> traffic
- Less attention is paid to deanonymize IP addresses involved in providing **hidden services**


## **Dataset Evaluation Criteria**

- **Covering Different Connections**
    - Tor
    - VPN
    - Tor over VPN
- **Complete Traffic**
    - diversity of protocols
    - diversity of applicaiton
- **Complete Interaction**
    - Aduio
    - Vedio
    - File transfer
    - Text/Chat
    - Email
    - VoIP
    - Web Browsing
    - P2P
- **Complete Capture**
    - Capturing header
    - Encrypted payload
    - without anonymization
- **Feature set**
>It represents features stored in the dataset. We classify this criterion as header and payload features to synchronize with complete capture mentioned above
- **Metadata**
> It means that details about dataset like captured traffic, attack scenario, type of protocols etc. are made available


:::info
**Despite containing encrypted or anonymized traffic, none of these datasets is complete in one way or the other to detect and characterize darknet traffic except ISCXVPN2016 and ISCXTor2017 that fulfil most of the defined criteria.**
:::




## **DATASET**

![](https://i.imgur.com/3Ap4ogE.png)



**結合 ISCXVPN2016 [27] 及 ISCXTor2017 [36]**

:::info
**ISCXVPN2016[27]**
The traffic was captured using **Wireshark and tcpdump**, generating a total amount of 28GB of data. For the VPN, we used an **external VPN service provider and connected to it using OpenVPN (UDP mode)**. To generate SFTP and FTPS traffic we also used an external service provider and Filezilla as a client.
:::

:::info
**ISCXTor2017[36]**
**Whonix OS**

we confirmed that the majority of traffic flows were generated by application X (skype, ftps, etc.), the object of the traffic capture. Then, we labelled all flows from the Tor .pcap file as X.

![](https://i.imgur.com/ojbJDOM.png)
:::

![](https://i.imgur.com/wc6xdgt.png)

![](https://i.imgur.com/DDxhREu.png)


### **Use PCAP File to generate feature csv file**

[Use this open source generator](https://github.com/ahlashkari/CICFlowMeter/blob/master/ReadMe.txt)

##  Architecture

![](https://i.imgur.com/RpI8sf4.png)


---

## NCCU_1101_DL_Final Project

## Import
```python=
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as Data
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.utils.data.dataset import random_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import f_regression
from sklearn.model_selection import train_test_split
```

## Load Data and pre-processing

### **Check NaN** 

-  $\dfrac{Mean(Total Length of Fwd Packet) + Mean(Total Length of Bwd Packet)}{Mean(Flow Duration) * 10^{-6}}$
    
```python=
df = pd.read_csv('/content/drive/MyDrive/DL/Final Project/Darknet.csv')

df_check_nan = df.columns[df.isna().any()].tolist() 
print(df_check_nan) #['Flow Bytes/s'] 有NaN
df["Flow Bytes/s"].fillna( (df["Total Length of Fwd Packet"].mean() + df["Total Length of Bwd Packet"].mean()) / (df["Flow Duration"].mean() * 0.000001) , inplace=True)
```

### **Check Inf**

$Fwd Packets/s + Bwd Packets/s$

```python=
# [Flow Packets/s] Inf 替換成 Fwd Packets/s + Bwd Packets/s
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df["Flow Packets/s"].fillna( df["Fwd Packets/s"] + df["Bwd Packets/s"] , inplace=True)
```
### **Drop the column with all zero value**

```python=
df.describe() # Min, Max, Mean are 0
df = df.drop(columns={"Fwd Bytes/Bulk Avg", "Fwd Packet/Bulk Avg", "Fwd Bulk Rate Avg", "Bwd Bytes/Bulk Avg", "Subflow Bwd Packets", "Active Mean", "Active Std", "Active Max", "Active Min", "URG Flag Count", "CWE Flag Count", "ECE Flag Count", "Bwd URG Flags", "Fwd URG Flags", "Bwd PSH Flags"})
```
### **Convert IP**
[ENCODING IP ADDRESS AS A FEATUREFOR NETWORK INTRUSION DETECTION, West Lafayette, Indiana, December 2019, Purdue University](https://hammer.purdue.edu/articles/thesis/Encoding_IP_Address_as_a_Feature_for_Network_Intrusion_Detection/11307287)   
:::info
有多種處理IP方式
1. 轉成Binary
2. One-hot Encoding
3. 切成四個部分

**根據這篇Paper把IP切成四個部分效果會最好**

![](https://i.imgur.com/Zt0DZfq.png)

:::

```python=
# split into 4 different number
df[['Src IP1','Src IP2','Src IP3','Src IP4']] = df["Src IP"].str.split('.', expand=True)
df[['Dst IP1','Dst IP2','Dst IP3','Dst IP4']] = df["Dst IP"].str.split('.', expand=True)

# Drop the original columns
df = df.drop(columns={"Src IP", "Dst IP"})
```

### **Encoging Label**
```python=
# Encoging Label
print(pd.unique(df["Label1"])) # ['Non-Tor' 'NonVPN' 'Tor' 'VPN']
print(pd.unique(df["Label2"])) # ['AUDIO-STREAMING' 'Browsing' 'Chat' 'Email' 'File-Transfer''File-transfer' 'P2P' 'Video-Streaming' 'Audio-Streaming' 'Video-streaming' 'VOIP']

# 因為paper裡面有說有只有8個 裡面有大小寫但是是一樣的東西
df = df.replace({'Label2':{"Audio-Streaming": "AUDIO-STREAMING", "Video-streaming": "Video-Streaming", "File-transfer": "File-Transfer"}})

# print(pd.unique(df["Label2"])) #['AUDIO-STREAMING' 'Browsing' 'Chat' 'Email' 'File-Transfer' 'P2P' 'Video-Streaming' 'VOIP']
df = df.replace({'Label1':{"VPN": 0, "Tor": 1, "Non-Tor": 2, "NonVPN": 3}})
df = df.replace({'Label2':{"AUDIO-STREAMING" : 0, "Browsing" : 1, "Chat" : 2, "Email" : 3, "File-Transfer" : 4, "P2P" : 5, "Video-Streaming" : 6, "VOIP" : 7}})

print(pd.unique(df["Label1"]))
print(pd.unique(df["Label2"]))
```


## **Features Selection**
### **Use ```f_regression``` to calculate P-value**
- **Drop P-value > 0.05**
    - Total Bwd packets
    - Fwd Packet Length Mean
    - Bwd Header Length
    - Packet Length Max
    - Down/Up Ratio
    - Fwd Segment Size Avg
    - FWD Init Win Bytes
    - Dst IP1 **(常理不會Drop掉Dst IP)**

- Drop **P-value > 0.03** Bwd IAT Total
```python=
fr = f_regression(x, y1)[1]
print(df.columns[fr > 0.05])
#['Total Bwd packets', 'Fwd Packet Length Mean', 'Bwd Header Length', 'Packet Length Max', 'Down/Up Ratio', 'Fwd Segment Size Avg', 'FWD Init Win Bytes', 'Dst IP1']

print(df.columns[fr > 0.03]) #多'Bwd IAT Total'
df = df.drop(columns={'Total Bwd packets', 'Fwd Packet Length Mean', 'Bwd IAT Total', 'Bwd Header Length', 'Packet Length Max', 'Down/Up Ratio', 'Fwd Segment Size Avg', 'FWD Init Win Bytes', })
```
### Drop some columns
- Timestamp (**features多且較難處理其次是經驗上應該不太相關**)
- Flow ID(**因為是由Dst, Src IP Port組成**)

```python=
df = df.drop(columns={"Flow ID", "Timestamp"})
```

## Split Data and normalization then convert to tensor
```python=
# x_split 是原先df還沒做f_regression前複製一份(因為有兩個Label)
train = x_split.sample(frac=0.8,random_state=200) #random state is a seed value
test = x_split.drop(train.index)
print(len(train['Label1']))
print(len(test['Label1']))

# Train 
label_1_train = train['Label1']
label_2_train = train['Label2']
data_train =  train.drop(columns={'Label1', 'Label2'})

# Train Z-Scale
scale = StandardScaler() #z-scaler
data_train = pd.DataFrame(scale.fit_transform(data_train),columns=data_train.keys())

# Train to numpy
label_1_train = pd.DataFrame.to_numpy(label_1_train)
label_2_train = pd.DataFrame.to_numpy(label_2_train)
data_train = pd.DataFrame.to_numpy(data_train)

# Test
label_1_test = test['Label1']
label_2_test = test['Label2']
data_test =  test.drop(columns={'Label1', 'Label2'})

# Test Z-Scale
data_test = pd.DataFrame(scale.fit_transform(data_test),columns=data_test.keys())

label_1_test = pd.DataFrame.to_numpy(label_1_test)
label_2_test = pd.DataFrame.to_numpy(label_2_test)
data_test = pd.DataFrame.to_numpy(data_test)

torch_Tensor_data_train = torch.as_tensor(np.array(data_train).astype('float'))
torch_Tensor_L1_train = torch.as_tensor(np.array(label_1_train).astype('long'))
torch_Tensor_L2_train = torch.as_tensor(np.array(label_2_train).astype('long'))

torch_Tensor_data_test = torch.as_tensor(np.array(data_test).astype('float'))
torch_Tensor_L1_test = torch.as_tensor(np.array(label_1_test).astype('float')).type(torch.LongTensor)
torch_Tensor_L2_test = torch.as_tensor(np.array(label_2_test).astype('float')).type(torch.LongTensor)

train = torch.utils.data.TensorDataset(torch_Tensor_data_train, torch_Tensor_L1_train, torch_Tensor_L2_train)
test = torch.utils.data.TensorDataset(torch_Tensor_data_test, torch_Tensor_L1_test, torch_Tensor_L2_test)

BATCH_SIZE = 64

train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)
test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)
```


## Build Model
### Fully Connected Network

```python=
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()

        self.fc1 = nn.Linear(64, 100, dtype=torch.float64)
        self.fc2 = nn.Linear(100, 150, dtype=torch.float64)
        self.fc3 = nn.Linear(150, 90, dtype=torch.float64)
        self.fc4 = nn.Linear(90, 50, dtype=torch.float64)
        self.fc5 = nn.Linear(50, 20, dtype=torch.float64)
        # 有4種類型流量 跟 8種服務型態
        self.fc6 = nn.Linear(20, 12, dtype=torch.float64)

    def forward(self, input):
        output = F.relu(self.fc1(input))      
        output = F.relu(self.fc2(output))
        output = F.relu(self.fc3(output))
        output = F.relu(self.fc4(output))
        output = F.relu(self.fc5(output))            
        output = F.sigmoid(self.fc6(output))
        return output

fc = NeuralNetwork().to(device)
```
### CNN Model

```python=
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=2)
        self.conv2 = nn.Conv2d(32, 32, kernel_size=2)
        self.conv3 = nn.Conv2d(32,64, kernel_size=2)
        self.fc1 = nn.Linear(1*8*8, 256)
        self.fc2 = nn.Linear(256, 12)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        #x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(F.max_pool2d(self.conv3(x),2))
        x = F.dropout(x, p=0.5, training=self.training)
        x = x.view(-1,1*8*8 )
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


cnn = Model().to(device)
print(cnn)

```


## Train
```python=
it = iter(train_loader)
X_batch, Y1_batch, Y2_batch = next(it)

def fit(model, train_loader):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    error = nn.CrossEntropyLoss()
    dataset_size = len(train_loader.dataset)
    EPOCHS = 10
    model.train()
    loss1_epoch = []
    loss2_epoch = []
    total_loss_epoch = []

    loss1_batch = []
    loss2_batch = []
    total_loss_batch = []
    for epoch in range(EPOCHS):
        correct = 0
        correct1 = 0
        correct2 = 0
        for batch_idx, (X_batch, Y1_batch, Y2_batch) in enumerate(train_loader):
            var_X_batch = X_batch.to(device)
            var_Y1_batch = Y1_batch.to(device)
            var_Y2_batch = Y2_batch.to(device)

            optimizer.zero_grad()

            output = model(var_X_batch)

            output = torch.split(output, [4,8], dim=1)
            #output = output.split(4,dim=1)


            loss = error(output[0], var_Y1_batch) + error(output[1], var_Y2_batch)
            
            output1 = torch.max(output[0], 1)[1]
            output2 = torch.max(output[1], 1)[1]
            loss.backward()
            optimizer.step()

            # Total correct predictions
            # predicted = torch.max(output.data, 1)[1] 
            correct1 += (output1 == var_Y1_batch).sum()
            correct2 += (output2 == var_Y2_batch).sum()
            correct = correct + (output1 == var_Y1_batch).sum() + (output2 == var_Y2_batch).sum()
            #print(correct)
            if batch_idx % 50 == 0:
                loss1_batch.append(error(output[0], var_Y1_batch))
                loss2_batch.append(error(output[1], var_Y2_batch))
                total_loss_batch.append( error(output[0], var_Y1_batch) + error(output[1], var_Y2_batch))   
                print('Epoch : {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\t Label1 Accuracy:{:.3f}%\t Label2 Accuracy:{:.3f}%\tAccuracy:{:.3f}%'.format(
                    epoch, batch_idx*BATCH_SIZE, dataset_size, 100.*batch_idx / len(train_loader),
                    loss.item(), float(correct1*100) / float(BATCH_SIZE*(batch_idx+1)), float(correct2*100) / float(BATCH_SIZE*(batch_idx+1)), float(correct*100) / float(2*BATCH_SIZE*(batch_idx+1))))
        loss1_epoch.append(error(output[0], var_Y1_batch))
        loss2_epoch.append(error(output[1], var_Y2_batch))
        total_loss_epoch.append( error(output[0], var_Y1_batch) + error(output[1], var_Y2_batch))     
                

    plt.plot(loss1_epoch, label = "4 Type")
    plt.plot(loss2_epoch, label = "8 Application")
    plt.plot(total_loss_epoch, label = "Total")
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.title('Fully connected Network Loss Epoch')
    plt.show()

    plt.plot(loss1_batch, label = "4 Type")
    plt.plot(loss2_batch, label = "8 Application")
    plt.plot(total_loss_batch, label = "Total")
    plt.ylabel('Loss')
    plt.xlabel('Batch')
    plt.title('Fully connected Network Loss Batch')
    plt.show()        
```

## Test
```python=
def test_model(model):
    model.eval()
    correct = 0
    correct1 = 0
    correct2 = 0
    for batch_idx, (test_x, test_L1, test_L2) in enumerate(test_loader):
        test_L1 = test_L1.to(device)
        test_L2 = test_L2.to(device)
        test_x = test_x.to(device)

        output = model(test_x)
        output = torch.split(output, [4,8], dim=1)

        output1 = torch.max(output[0], 1)[1]
        output2 = torch.max(output[1], 1)[1]

        correct1 += (output1 == test_L1).sum()
        correct2 += (output2 == test_L2).sum()
        correct = correct + ( (output1 == test_L1).sum() + (output2 == test_L2).sum() )

    print('Label1 Accuracy:{:.3f}%\t Label2 Accuracy:{:.3f}%\tAccuracy:{:.3f}%'.format(float(correct1*100) / float(BATCH_SIZE*(batch_idx+1)), float(correct2*100) / float(BATCH_SIZE*(batch_idx+1)), float(correct*100) / float(2*BATCH_SIZE*(batch_idx+1))))

test_model(fc)
```

## Performance 

### FC
<span style="color:red">**Train**</span>
![](https://i.imgur.com/omgXMId.png)

<span style="color:red">**Train_Epoch**</span>
![](https://i.imgur.com/tprk23P.png)


<span style="color:red">**Train_Batch**</span>
![](https://i.imgur.com/deQNGLk.png)
:::info
Blue : "4 Type"
Orange : "8 Application"
Green : "Total"
:::


<span style="color:red">**Test**</span>
![](https://i.imgur.com/EhSoZvP.png)


---

### CNN
<span style="color:red">**Train**</span>
![](https://i.imgur.com/oj8fwK6.png)

<span style="color:red">**Train_Epoch**</span>
![](https://i.imgur.com/FmcTQCb.png)

<span style="color:red">**Train_Batch**</span>
![](https://i.imgur.com/DUEzqZY.png)

:::info
Blue : "4 Type"
Orange : "8 Application"
Green : "Total"
:::

<span style="color:red">**Test**</span>
![](https://i.imgur.com/lSb4JG0.png)

## Paper :  Classification(8 Applicaiton) : Accuracy 0.76375

![](https://i.imgur.com/ZepiZjT.png)


## 參考資料
[Arash Habibi Lashkari, Gurdip Kaur and Abir Rahali, "DIDarknet: A Contemporary Approach to Detect and Characterize the Darknet Traffic using Deep Image Learning", 10th International Conference on Communication and Network Security, November 2020.](https://dl.acm.org/doi/10.1145/3442520.3442521)

[L. A. Iliadis and T. Kaifas, "Darknet Traffic Classification using Machine Learning Techniques," 2021 10th International Conference on Modern Circuits and Systems Technologies (MOCAST), 2021, pp. 1-4, doi: 10.1109/MOCAST52088.2021.9493386.](https://ieeexplore.ieee.org/document/9493386/references#references)



## 組內分工
- **期中發想**(110753113)
- **文獻探討**(110753113)
- **實驗設計**(110753113)
- **程式撰寫**(110753113)
- **書面報告**(110753113)
- **線上報告**(110753113)
